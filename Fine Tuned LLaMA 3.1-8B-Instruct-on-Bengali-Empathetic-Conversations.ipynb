{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceType":"datasetVersion","sourceId":6104837,"datasetId":3497143,"databundleVersionId":6183410}],"dockerImageVersionId":31286,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **installation**","metadata":{}},{"cell_type":"code","source":"!pip install -U transformers accelerate peft datasets evaluate bitsandbytes sentencepiece trl","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:31:43.815949Z","iopub.execute_input":"2026-02-25T09:31:43.816176Z","iopub.status.idle":"2026-02-25T09:31:56.278778Z","shell.execute_reply.started":"2026-02-25T09:31:43.816154Z","shell.execute_reply":"2026-02-25T09:31:56.278119Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.1)\nRequirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\nCollecting datasets\n  Downloading datasets-4.5.0-py3-none-any.whl.metadata (19 kB)\nCollecting evaluate\n  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\nCollecting bitsandbytes\n  Downloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: sentencepiece in /usr/local/lib/python3.12/dist-packages (0.2.1)\nCollecting trl\n  Downloading trl-0.28.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: huggingface-hub<2.0,>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (1.4.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\nRequirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\nRequirement already satisfied: typer-slim in /usr/local/lib/python3.12/dist-packages (from transformers) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from accelerate) (5.9.5)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from accelerate) (2.9.0+cu126)\nRequirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.20.3)\nCollecting pyarrow>=21.0.0 (from datasets)\n  Downloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.1 kB)\nRequirement already satisfied: dill<0.4.1,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.3.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\nRequirement already satisfied: httpx<1.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.28.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.6.0)\nRequirement already satisfied: multiprocess<0.70.19 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.10.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (3.13.3)\nRequirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (4.12.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (2026.1.4)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (1.0.9)\nRequirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<1.0.0->datasets) (3.11)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\nRequirement already satisfied: hf-xet<2.0.0,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.2.0)\nRequirement already satisfied: shellingham in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (1.5.4)\nRequirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=1.3.0->transformers) (4.15.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (75.2.0)\nRequirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.14.0)\nRequirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.6.1)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.80)\nRequirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\nRequirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.4.1)\nRequirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.3.0.4)\nRequirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (10.3.7.77)\nRequirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (11.7.1.2)\nRequirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.5.4.2)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (0.7.1)\nRequirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (2.27.5)\nRequirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.3.20)\nRequirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.77)\nRequirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (12.6.85)\nRequirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (1.11.1.6)\nRequirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0.0->accelerate) (3.5.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.3)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from typer-slim->transformers) (8.3.1)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.4.0)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.4.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.8.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (6.7.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (0.4.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (1.22.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.3)\nDownloading datasets-4.5.0-py3-none-any.whl (515 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m515.2/515.2 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.49.2-py3-none-manylinux_2_24_x86_64.whl (60.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.7/60.7 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.28.0-py3-none-any.whl (540 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.5/540.5 kB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pyarrow-23.0.1-cp312-cp312-manylinux_2_28_x86_64.whl (47.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.6/47.6 MB\u001b[0m \u001b[31m41.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: pyarrow, datasets, bitsandbytes, evaluate, trl\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 18.1.0\n    Uninstalling pyarrow-18.1.0:\n      Successfully uninstalled pyarrow-18.1.0\n  Attempting uninstall: datasets\n    Found existing installation: datasets 4.0.0\n    Uninstalling datasets-4.0.0:\n      Successfully uninstalled datasets-4.0.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.31.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ngoogle-adk 1.21.0 requires google-cloud-bigquery-storage>=2.0.0, which is not installed.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.49.2 datasets-4.5.0 evaluate-0.4.6 pyarrow-23.0.1 trl-0.28.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# **libraries**","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nimport pandas as pd\nfrom datasets import Dataset\nimport random\nimport evaluate\nimport math\n\ntorch.backends.cuda.matmul.fp32_precision = \"tf32\"\ntorch.backends.cudnn.conv.fp32_precision = \"tf32\"\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForCausalLM,\n    BitsAndBytesConfig,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForLanguageModeling\n)\n\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:31:56.280422Z","iopub.execute_input":"2026-02-25T09:31:56.280678Z","iopub.status.idle":"2026-02-25T09:32:16.838320Z","shell.execute_reply.started":"2026-02-25T09:31:56.280652Z","shell.execute_reply":"2026-02-25T09:32:16.837553Z"}},"outputs":[],"execution_count":2},{"cell_type":"markdown","source":"# **log **","metadata":{}},{"cell_type":"code","source":"import json\nimport uuid\nfrom datetime import datetime\n\ndef log_experiment(model_name, lora_config, train_loss, val_loss, metrics):\n\n    experiment = {\n        \"id\": str(uuid.uuid4()),\n        \"model_name\": model_name,\n        \"lora_config\": lora_config,\n        \"train_loss\": train_loss,\n        \"val_loss\": val_loss,\n        \"metrics\": metrics,\n        \"timestamp\": str(datetime.now())\n    }\n\n    with open(\"/kaggle/working/LLAMAExperiments.json\", \"a\") as f:\n        f.write(json.dumps(experiment) + \"\\n\")\n\n    return experiment[\"id\"]\n\n\ndef log_generated_response(experiment_id, input_text, response_text):\n\n    record = {\n        \"id\": str(uuid.uuid4()),\n        \"experiment_id\": experiment_id,\n        \"input_text\": input_text,\n        \"response_text\": response_text,\n        \"timestamp\": str(datetime.now())\n    }\n\n    with open(\"/kaggle/working/GeneratedResponses.json\", \"a\") as f:\n        f.write(json.dumps(record) + \"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:32:16.839246Z","iopub.execute_input":"2026-02-25T09:32:16.839756Z","iopub.status.idle":"2026-02-25T09:32:16.845107Z","shell.execute_reply.started":"2026-02-25T09:32:16.839730Z","shell.execute_reply":"2026-02-25T09:32:16.844525Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\nlogin()  # token of your hugging face ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:14:03.367791Z","iopub.execute_input":"2026-02-25T08:14:03.368024Z","iopub.status.idle":"2026-02-25T08:14:03.643021Z","shell.execute_reply.started":"2026-02-25T08:14:03.368001Z","shell.execute_reply":"2026-02-25T08:14:03.642277Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8df3caf69f864266b220ce12c08e0c38"}},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"# ==============================\n# Strategy Pattern - Interface\n# ==============================\n\nclass FineTuningStrategy:\n    def apply(self, base_model):\n        raise NotImplementedError(\"Strategy must implement apply() method.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:14:16.344190Z","iopub.execute_input":"2026-02-25T08:14:16.344534Z","iopub.status.idle":"2026-02-25T08:14:16.348300Z","shell.execute_reply.started":"2026-02-25T08:14:16.344503Z","shell.execute_reply":"2026-02-25T08:14:16.347726Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ==============================\n# LoRA Strategy\n# ==============================\n\nfrom peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n\nclass LoRAStrategy(FineTuningStrategy):\n    def __init__(self, r=8, alpha=16, dropout=0.05):\n        self.r = r\n        self.alpha = alpha\n        self.dropout = dropout\n\n    def apply(self, base_model):\n\n        # Required for 4-bit training\n        base_model = prepare_model_for_kbit_training(base_model)\n\n        lora_config = LoraConfig(\n            r=self.r,\n            lora_alpha=self.alpha,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n            lora_dropout=self.dropout,\n            bias=\"none\",\n            task_type=\"CAUSAL_LM\"\n        )\n\n        model = get_peft_model(base_model, lora_config)\n\n        print(\"LoRA Strategy Applied\")\n        model.print_trainable_parameters()\n\n        return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:14:18.809978Z","iopub.execute_input":"2026-02-25T08:14:18.810587Z","iopub.status.idle":"2026-02-25T08:14:18.815802Z","shell.execute_reply.started":"2026-02-25T08:14:18.810547Z","shell.execute_reply":"2026-02-25T08:14:18.814946Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ==============================\n# Unsloth Strategy (Skeleton)\n# ==============================\n\nclass UnslothStrategy(FineTuningStrategy):\n    def apply(self, base_model):\n        raise NotImplementedError(\"Unsloth strategy not implemented yet.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:14:23.600219Z","iopub.execute_input":"2026-02-25T08:14:23.600842Z","iopub.status.idle":"2026-02-25T08:14:23.604588Z","shell.execute_reply.started":"2026-02-25T08:14:23.600800Z","shell.execute_reply":"2026-02-25T08:14:23.603957Z"}},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":"# **Data Preprocessor and LLama fine tuner Class**","metadata":{}},{"cell_type":"code","source":"import re\nfrom datasets import Dataset\n\nUNSAFE_PATTERNS = [\n    r\"ড্রিঙ্ক\",\n    r\"চা খা\",\n    r\"চলে আস\",\n    r\"দেখা কর\",\n    r\"ঈশ্বর\",\n    r\"আল্লাহ\",\n]\n\nREFLECTIVE_PREFIXES = [\n    \"শুনে মনে হচ্ছে আপনার জন্য বিষয়টি সত্যিই কঠিন লাগছে। \",\n    \"এই অনুভূতিটা খুব ভারী হতে পারে, এবং তা বোঝা যায়। \",\n    \"আপনি যা অনুভব করছেন, তা অনেকের জীবনে আসতে পারে। \",\n    \"এই মুহূর্তে আপনার ভেতরে অনেক চাপ কাজ করছে বলে মনে হচ্ছে। \",\n]\n\n\nclass DatasetProcessor:\n    def __init__(self, path, sample_size=None):\n        self.df = pd.read_csv(path)[[\"Questions\", \"Answers\"]]\n        self.sample_size = sample_size\n\n    def normalize_answer(self, answer: str) -> str:\n        answer = str(answer)\n\n        for pat in UNSAFE_PATTERNS:\n            answer = re.sub(pat, \"\", answer)\n\n        sentences = re.split(r'(?<=[।!?])', answer)\n        sentences = [s.strip() for s in sentences if len(s.strip()) > 3]\n\n        return \" \".join(sentences)\n\n    def add_reflection(self, answer: str) -> str:\n        if any(answer.startswith(p) for p in REFLECTIVE_PREFIXES):\n            return answer\n        return random.choice(REFLECTIVE_PREFIXES) + answer\n\n    def build_text(self, row):\n        question = str(row[\"Questions\"]).strip()\n        answer = self.normalize_answer(row[\"Answers\"])\n        answer = self.add_reflection(answer)\n\n        return (\n            \"নির্দেশনা:\\n\"\n            \"ব্যবহারকারীর কথার প্রতি সহানুভূতি প্রকাশ করে উত্তর দিন। \"\n            \"প্রথমে অনুভূতি স্বীকার করুন, বিচার না করে সমর্থনমূলক ভাষা ব্যবহার করুন, \"\n            \"এবং কোমলভাবে সাহায্য করার চেষ্টা করুন。\\n\\n\"\n            \"ব্যবহারকারীর কথা:\\n\"\n            f\"{question}\\n\\n\"\n            \"উত্তর:\\n\"\n            f\"{answer}\"\n        )\n\n    def process(self):\n        self.df[\"text\"] = self.df.apply(self.build_text, axis=1)\n        dataset = Dataset.from_pandas(self.df[[\"text\"]])\n\n        if self.sample_size:\n            dataset = dataset.shuffle(seed=42).select(range(self.sample_size))\n\n        return dataset\n\n\nclass LLAMAFineTuner:\n    def __init__(self, base_model, tokenizer, strategy: FineTuningStrategy):\n        self.base_model = base_model\n        self.tokenizer = tokenizer\n        self.strategy = strategy\n        self.model = None\n\n    def load_model(self):\n\n        # QLoRA 4-bit config\n        bnb_config = BitsAndBytesConfig(\n            load_in_4bit=True,\n            bnb_4bit_use_double_quant=True,\n            bnb_4bit_quant_type=\"nf4\",\n            bnb_4bit_compute_dtype=torch.float16,\n        )\n\n        base_model = AutoModelForCausalLM.from_pretrained(\n            self.base_model,\n            quantization_config=bnb_config,\n            device_map=\"auto\",\n            torch_dtype=torch.float16,\n        )\n\n        # Gradient checkpointing\n        base_model.gradient_checkpointing_enable()\n        base_model.config.use_cache = False\n\n        # Apply selected strategy (LoRA / Unsloth)\n        self.model = self.strategy.apply(base_model)\n\n        return self.model\n\n    def train(self, tokenized_train, tokenized_val,\n              epochs=1, batch_size=1, gradient_accumulation_steps=8):\n\n        data_collator = DataCollatorForLanguageModeling(\n            tokenizer=self.tokenizer,\n            mlm=False\n        )\n\n        training_args = TrainingArguments(\n            output_dir=\"./llama_bengali\",\n            per_device_train_batch_size=batch_size,\n            gradient_accumulation_steps=gradient_accumulation_steps,\n            num_train_epochs=epochs,\n            learning_rate=2e-4,\n            fp16=True,\n            logging_steps=10,\n            eval_strategy=\"epoch\",\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"loss\",\n            report_to=\"none\",\n            remove_unused_columns=False,\n        )\n\n        trainer = Trainer(\n            model=self.model,\n            args=training_args,\n            train_dataset=tokenized_train,\n            eval_dataset=tokenized_val,\n            data_collator=data_collator\n        )\n\n        trainer.train()\n        return trainer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:14:25.754870Z","iopub.execute_input":"2026-02-25T08:14:25.755442Z","iopub.status.idle":"2026-02-25T08:14:25.766887Z","shell.execute_reply.started":"2026-02-25T08:14:25.755389Z","shell.execute_reply":"2026-02-25T08:14:25.766246Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"# **Dataset & Model Load**","metadata":{}},{"cell_type":"code","source":"DATA_PATH = \"/kaggle/input/datasets/raseluddin/bengali-empathetic-conversations-corpus/BengaliEmpatheticConversationsCorpus .csv\"\nprocessor = DatasetProcessor(DATA_PATH, sample_size=600)\n\ndataset = processor.process()\ndataset = dataset.train_test_split(test_size=0.2, seed=42)\n\ntrain_dataset = dataset[\"train\"]\nval_dataset   = dataset[\"test\"]\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8b-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:53:03.516220Z","iopub.execute_input":"2026-02-25T08:53:03.517009Z","iopub.status.idle":"2026-02-25T08:53:07.054328Z","shell.execute_reply.started":"2026-02-25T08:53:03.516971Z","shell.execute_reply":"2026-02-25T08:53:07.053565Z"}},"outputs":[],"execution_count":21},{"cell_type":"markdown","source":"# **Train and Test split**","metadata":{}},{"cell_type":"code","source":"def tokenize_fn(batch):\n    enc = tokenizer(\n        batch[\"text\"],\n        truncation=True,\n        max_length=1024,\n        padding=\"max_length\"\n    )\n\n    # Important: convert to list before copying\n    enc[\"labels\"] = enc[\"input_ids\"].copy()\n\n    return enc\n\n\ntokenized_train = train_dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\ntokenized_val = val_dataset.map(\n    tokenize_fn,\n    batched=True,\n    remove_columns=[\"text\"]\n)\n\n\n\nprint(\"Tokenized train examples:\", len(tokenized_train))\nprint(\"Tokenized val examples:\", len(tokenized_val))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:53:07.055728Z","iopub.execute_input":"2026-02-25T08:53:07.055983Z","iopub.status.idle":"2026-02-25T08:53:07.897519Z","shell.execute_reply.started":"2026-02-25T08:53:07.055960Z","shell.execute_reply":"2026-02-25T08:53:07.896759Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/480 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a573c596b2484f7e9012ebc3181f9465"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/120 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a7667a02e8264b1597f180ea2835b96a"}},"metadata":{}},{"name":"stdout","text":"Tokenized train examples: 480\nTokenized val examples: 120\n","output_type":"stream"}],"execution_count":22},{"cell_type":"code","source":"import torch, gc\ngc.collect()\ntorch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T10:04:52.214458Z","iopub.execute_input":"2026-02-25T10:04:52.215174Z","iopub.status.idle":"2026-02-25T10:04:52.546208Z","shell.execute_reply.started":"2026-02-25T10:04:52.215142Z","shell.execute_reply":"2026-02-25T10:04:52.545246Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# **Model Training**","metadata":{}},{"cell_type":"code","source":"# ==============================\n# Initialize Strategy + Tuner\n# ==============================\n\nMODEL_NAME = \"meta-llama/Llama-3.1-8b-Instruct\"\n\nstrategy = LoRAStrategy(r=8, alpha=16)\n\ntuner = LLAMAFineTuner(\n    base_model=MODEL_NAME,\n    tokenizer=tokenizer,\n    strategy=strategy\n)\n\nmodel = tuner.load_model()\nprint(\"Model loaded using Strategy Pattern.\")\n\ntrainer = tuner.train(\n    tokenized_train,\n    tokenized_val,\n    epochs=2,\n    batch_size=1,\n    gradient_accumulation_steps=2\n)\n\nprint(\"Training finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T08:53:12.916222Z","iopub.execute_input":"2026-02-25T08:53:12.916998Z","iopub.status.idle":"2026-02-25T09:23:03.232694Z","shell.execute_reply.started":"2026-02-25T08:53:12.916965Z","shell.execute_reply":"2026-02-25T09:23:03.231487Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91cfe7d529b64845b5dc6bccfdf22704"}},"metadata":{}},{"name":"stdout","text":"LoRA Strategy Applied\ntrainable params: 6,815,744 || all params: 8,037,076,992 || trainable%: 0.0848\nModel loaded using Strategy Pattern.\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n  return fn(*args, **kwargs)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='241' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [241/480 29:22 < 29:22, 0.14 it/s, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2619408780.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model loaded using Strategy Pattern.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m trainer = tuner.train(\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mtokenized_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mtokenized_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_55/52696058.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, tokenized_train, tokenized_val, epochs, batch_size, gradient_accumulation_steps)\u001b[0m\n\u001b[1;32m    131\u001b[0m         )\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1410\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1412\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   1413\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m             self._maybe_log_save_evaluate(\n\u001b[0m\u001b[1;32m   1848\u001b[0m                 \u001b[0mtr_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_maybe_log_save_evaluate\u001b[0;34m(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, start_time, learning_rate)\u001b[0m\n\u001b[1;32m   2123\u001b[0m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2124\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_evaluate\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2125\u001b[0;31m             \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2126\u001b[0m             \u001b[0mis_new_best_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_determine_best_metric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_evaluate\u001b[0;34m(self, trial, ignore_keys_for_eval, skip_scheduler)\u001b[0m\n\u001b[1;32m   2981\u001b[0m     ) -> dict[str, float]:\n\u001b[1;32m   2982\u001b[0m         \u001b[0;34m\"\"\"Run evaluation, report to HP search, and step ReduceLROnPlateau if needed.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2983\u001b[0;31m         \u001b[0mmetrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys_for_eval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2984\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_report_to_hp_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2566\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2567\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2568\u001b[0;31m         output = self.evaluation_loop(\n\u001b[0m\u001b[1;32m   2569\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2570\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2694\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2695\u001b[0;31m             \u001b[0mlosses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2696\u001b[0m             \u001b[0mmain_input_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"main_input_name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2697\u001b[0m             inputs_decode = (\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   2947\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2948\u001b[0m                         \u001b[0mnum_items_in_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_num_items_in_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2949\u001b[0;31m                         loss, outputs = self.compute_loss(\n\u001b[0m\u001b[1;32m   2950\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2951\u001b[0m                         )\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[1;32m   2020\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"num_items_in_batch\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_items_in_batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2021\u001b[0m             \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2022\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2023\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2024\u001b[0m         \u001b[0;31m# User-defined compute_loss function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     decorate_autocast.__script_unsupported = (  # type: ignore[attr-defined]\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1921\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_enable_peft_forward_hooks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m                 \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecial_peft_forward_args\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1924\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1925\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1773\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1774\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1775\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1776\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1777\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1784\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1785\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1786\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1788\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/accelerate/hooks.py\u001b[0m in \u001b[0;36mnew_forward\u001b[0;34m(module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_old_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_hf_hook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/generic.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    840\u001b[0m             \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict_passed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 841\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    843\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/llama/modeling_llama.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, cache_position, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 505\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m         return CausalLMOutputWithPast(\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/loss/loss_utils.py\u001b[0m in \u001b[0;36mForCausalLMLoss\u001b[0;34m(logits, labels, vocab_size, num_items_in_batch, ignore_index, shift_labels, **kwargs)\u001b[0m\n\u001b[1;32m     53\u001b[0m ) -> torch.Tensor:\n\u001b[1;32m     54\u001b[0m     \u001b[0;31m# Upcast to float if we need to compute the loss to avoid potential precision issues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshift_labels\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 3.91 GiB. GPU 1 has a total capacity of 14.56 GiB of which 369.81 MiB is free. Including non-PyTorch memory, this process has 14.20 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"],"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 3.91 GiB. GPU 1 has a total capacity of 14.56 GiB of which 369.81 MiB is free. Including non-PyTorch memory, this process has 14.20 GiB memory in use. Of the allocated memory 11.21 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error"}],"execution_count":24},{"cell_type":"code","source":"# After trainer.train() finishes\ntrain_loss = trainer.state.log_history[-1][\"loss\"]  # approximate\nval_loss = trainer.state.best_metric          # if available\n\nlora_config = {\n    \"r\": 8,\n    \"alpha\": 16,\n    \"dropout\": 0.05\n}\n\nexperiment_id = log_experiment(\n    model_name=MODEL_NAME,\n    lora_config=lora_config,\n    train_loss=train_loss,\n    val_loss=val_loss,\n    metrics={\"BLEU\": 0, \"ROUGE-L\": 0}  # placeholder if metrics not computed yet\n)\nprint(\"Experiment logged with ID:\", experiment_id)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# **Save model**","metadata":{}},{"cell_type":"code","source":"from huggingface_hub import logout, login, whoami\n\n#model save \nlogout()\nlogin()\n\nwhoami()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:32:16.848504Z","iopub.execute_input":"2026-02-25T09:32:16.848697Z","iopub.status.idle":"2026-02-25T09:32:17.466093Z","shell.execute_reply.started":"2026-02-25T09:32:16.848678Z","shell.execute_reply":"2026-02-25T09:32:17.465373Z"}},"outputs":[{"name":"stderr","text":"Not logged in!\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"{'type': 'user',\n 'id': '67c81ad55993d755c95e4558',\n 'name': 'Fariha1999',\n 'fullname': 'Fariha Tasnim Chowdhury',\n 'email': 'farihatasnimchowdhury2024@gmail.com',\n 'emailVerified': True,\n 'canPay': False,\n 'billingMode': 'prepaid',\n 'periodEnd': 1772323200,\n 'isPro': False,\n 'avatarUrl': 'https://cdn-avatars.huggingface.co/v1/production/uploads/no-auth/oqIcbDW8u03qOgHsN9fRD.png',\n 'orgs': [],\n 'auth': {'type': 'access_token',\n  'accessToken': {'displayName': 'fariha',\n   'role': 'write',\n   'createdAt': '2026-02-21T15:57:21.879Z'}}}"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"from huggingface_hub import create_repo\n\nHF_REPO = \"your hugging face repo\"\n\ncreate_repo(\n    repo_id=HF_REPO,\n    repo_type=\"model\",\n    private=True,      # recommended\n    exist_ok=True\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:23:18.037379Z","iopub.execute_input":"2026-02-25T09:23:18.037709Z","iopub.status.idle":"2026-02-25T09:23:18.395805Z","shell.execute_reply.started":"2026-02-25T09:23:18.037683Z","shell.execute_reply":"2026-02-25T09:23:18.395065Z"}},"outputs":[{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"RepoUrl('https://huggingface.co/Fariha1999/llama-3.1-8b-bengali-empathetic-lora7', endpoint='https://huggingface.co', repo_type='model', repo_id='Fariha1999/llama-3.1-8b-bengali-empathetic-lora7')"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"model.push_to_hub(HF_REPO)\ntokenizer.push_to_hub(HF_REPO)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:23:21.726873Z","iopub.execute_input":"2026-02-25T09:23:21.727772Z","iopub.status.idle":"2026-02-25T09:23:26.311231Z","shell.execute_reply.started":"2026-02-25T09:23:21.727727Z","shell.execute_reply":"2026-02-25T09:23:26.310478Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18c613d4166346c199512921892a9416"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c58aa5ede2de43fd8c8a2fc35b1c538d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/5.17k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df1082a030fc4cf79618c0b57fe9ae50"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Processing Files (0 / 0): |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7798711010244e1b931b3be9c9360f7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"New Data Upload: |          |  0.00B /  0.00B            ","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9d61d03c13c41e9af2a691fb7ab6de1"}},"metadata":{}},{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Fariha1999/llama-3.1-8b-bengali-empathetic-lora7/commit/ffb55867f612bfed6d69980867363cbb8c169312', commit_message='Upload tokenizer', commit_description='', oid='ffb55867f612bfed6d69980867363cbb8c169312', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Fariha1999/llama-3.1-8b-bengali-empathetic-lora7', endpoint='https://huggingface.co', repo_type='model', repo_id='Fariha1999/llama-3.1-8b-bengali-empathetic-lora7'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":27},{"cell_type":"markdown","source":"# **Evaluation**","metadata":{}},{"cell_type":"code","source":"# -------------------------------\n# 2️⃣ Define Evaluator class\n# -------------------------------\nclass Evaluator:\n    def __init__(self, model, tokenizer, val_dataset, experiment_id):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.val_dataset = val_dataset\n        self.experiment_id = experiment_id\n        self.bleu = evaluate.load(\"sacrebleu\")\n        self.rouge = evaluate.load(\"rouge\")\n\n    def compute_perplexity(self):\n        self.model.eval()\n        losses = []\n\n        for batch in self.val_dataset:\n            input_ids = torch.tensor(batch[\"input_ids\"]).unsqueeze(0).to(self.model.device)\n            labels = input_ids.clone()\n\n            with torch.no_grad():\n                outputs = self.model(input_ids=input_ids, labels=labels)\n                loss = outputs.loss\n\n            losses.append(loss.item())\n\n        avg_loss = sum(losses) / len(losses)\n        return math.exp(avg_loss)\n\n    def compute_generation_metrics(self, max_samples=50):\n        predictions = []\n        references = []\n\n        for i in range(min(max_samples, len(self.val_dataset))):\n            input_ids = self.val_dataset[i][\"input_ids\"]\n            text = self.tokenizer.decode(input_ids, skip_special_tokens=True)\n\n            # Split question and reference\n            question = text.split(\"উত্তর:\")[0]\n            reference = text.split(\"উত্তর:\")[-1]\n\n            # Generate response\n            generated = generate_response(self.model, self.tokenizer, question)\n\n            # ---- Logging generated response ----\n            log_generated_response(\n                experiment_id=self.experiment_id,\n                input_text=question,\n                response_text=generated\n            )\n\n            predictions.append(generated)\n            references.append([reference])\n\n        bleu_score = self.bleu.compute(predictions=predictions, references=references)\n        rouge_score = self.rouge.compute(predictions=predictions, references=[r[0] for r in references])\n\n        return {\n            \"BLEU\": bleu_score[\"score\"],\n            \"ROUGE-L\": rouge_score[\"rougeL\"],\n        }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T10:11:20.277550Z","iopub.execute_input":"2026-02-25T10:11:20.278249Z","iopub.status.idle":"2026-02-25T10:11:20.289541Z","shell.execute_reply.started":"2026-02-25T10:11:20.278216Z","shell.execute_reply":"2026-02-25T10:11:20.288720Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"# -------------------------------\n# 3️⃣ Log experiment (without trainer)\n# -------------------------------\n# Since the training session is gone, we pass None for train/val losses\nexperiment_id = log_experiment(\n    model_name=model_path,\n    lora_config={\"r\": 8, \"alpha\": 16},\n    train_loss=None,\n    val_loss=None,\n    metrics={}  # will update after evaluation\n)\n\n# -------------------------------\n# 4️⃣ Create evaluator instance\n# -------------------------------\nevaluator = Evaluator(model, tokenizer, tokenized_val, experiment_id)\n\n# -------------------------------\n# 5️⃣ Compute perplexity\n# -------------------------------\nperplexity = evaluator.compute_perplexity()\nprint(\"Perplexity:\", perplexity)\n\n# -------------------------------\n# 6️⃣ Compute BLEU / ROUGE and log generated responses\n# -------------------------------\nmetrics = evaluator.compute_generation_metrics(max_samples=50)\nprint(\"Metrics:\", metrics)\n\n# -------------------------------\n# 7️⃣ Optional: Update experiment JSON with metrics\n# -------------------------------\nupdate_experiment_metrics(experiment_id, metrics)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T10:12:32.079473Z","iopub.execute_input":"2026-02-25T10:12:32.080204Z","iopub.status.idle":"2026-02-25T10:12:32.089565Z","shell.execute_reply.started":"2026-02-25T10:12:32.080171Z","shell.execute_reply":"2026-02-25T10:12:32.088524Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_55/2106279268.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# 4️⃣ Create evaluator instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mevaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenized_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# -------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'tokenized_val' is not defined"],"ename":"NameError","evalue":"name 'tokenized_val' is not defined","output_type":"error"}],"execution_count":30},{"cell_type":"markdown","source":"# **Fine tuned model load**","metadata":{}},{"cell_type":"code","source":"import torch\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\nBASE_MODEL = \"meta-llama/Llama-3.1-8b-Instruct\"\nLORA_REPO = \"Fariha1999/llama-3.1-8b-bengali-empathetic-lora7\"\n\ntokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=False)\n\nbase_model = AutoModelForCausalLM.from_pretrained(\n    BASE_MODEL,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n    low_cpu_mem_usage=True\n)\n\nmodel = PeftModel.from_pretrained(\n    base_model,\n    LORA_REPO,\n    is_trainable=False\n)\n\nmodel.eval()\n\nprint(model.config.model_type)\nprint(tokenizer.name_or_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:32:17.467055Z","iopub.execute_input":"2026-02-25T09:32:17.467304Z","iopub.status.idle":"2026-02-25T09:33:42.331303Z","shell.execute_reply.started":"2026-02-25T09:32:17.467281Z","shell.execute_reply":"2026-02-25T09:33:42.330502Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8018c97f9964c01b27a041d3da98882"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54e403f242294fa99ee030cd06b8753c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13e9b72429e8437e8706f3a63f721ec4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a43f7812d72b4d758e196c9aa5321ca5"}},"metadata":{}},{"name":"stderr","text":"`torch_dtype` is deprecated! Use `dtype` instead!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"80eb2a6077e44972b6daa3f57d865a52"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading (incomplete total...): 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"949025a23ecd4032b2f6106863319cd7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b394adba9b464e40aa0a45f362503173"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading weights:   0%|          | 0/291 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"606463f40e6f4e008569bde340fe2215"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcdd6c2be15d4e359aba5da1f659ade0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"42514bcc52114d5787e166e0ae2b08da"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/27.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a0809e5555e24ae7bd869964a35dec58"}},"metadata":{}},{"name":"stdout","text":"llama\nmeta-llama/Llama-3.1-8b-Instruct\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"# **Response Generation**","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\nquestions = [\n    \"আমি সবসময় অনলাইনে সফল হলেও ভেতরে একা অনুভব করি। এর মানে কি আমার জীবনে কিছু ভুল আছে?\",\n    \"আমার বন্ধুরা আমাকে অবমূল্যায়ন করে। আমি কেমন প্রতিক্রিয়া দেখাই?\",\n    \"আমি অনেক চাপ অনুভব করছি এবং ঘুমও ঠিকমতো হচ্ছে না। আমি কি করব?\",\n    \"আমি যে কাজগুলো করি তাতে কোনো আনন্দ পাই না। এটি কি স্বাভাবিক?\",\n    \"আমি অপরাধবোধ বোধ করি যখন অন্যদের সাহায্য করতে পারি না। আমি কি ভুল করছি?\"\n]\n\ndf = pd.DataFrame({\"question\": questions})\ndf.to_csv(\"/kaggle/working/questions.csv\", index=False)\nprint(\"questions.csv saved!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:33:42.332219Z","iopub.execute_input":"2026-02-25T09:33:42.332778Z","iopub.status.idle":"2026-02-25T09:33:42.349102Z","shell.execute_reply.started":"2026-02-25T09:33:42.332750Z","shell.execute_reply":"2026-02-25T09:33:42.348415Z"}},"outputs":[{"name":"stdout","text":"questions.csv saved!\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"import torch\nimport pandas as pd\nfrom tqdm import tqdm\n\n# -------- CONFIG --------\nINPUT_PATH = \"/kaggle/working/questions.csv\"\nOUTPUT_PATH = \"/kaggle/working/model_generations.csv\"\n\nN_SAMPLES = 3          # Reduced from 5\nMAX_NEW_TOKENS = 300   # Reduced from 300\n# ------------------------\n\n# Make sure model is in eval mode and cache is used\nmodel.eval()\nmodel.config.use_cache = True\ntorch.set_grad_enabled(False)\n\ndef generate_responses(question, n_samples=N_SAMPLES):\n    prompt = (\n        \"নির্দেশনা:\\n\"\n        \"ব্যবহারকারীর কথার প্রতি সহানুভূতি প্রকাশ করে উত্তর দিন। \"\n        \"প্রথমে অনুভূতি স্বীকার করুন, বিচার না করে সমর্থনমূলক ভাষা ব্যবহার করুন, \"\n        \"এবং কোমলভাবে সাহায্য করার চেষ্টা করুন。\\n\\n\"\n        f\"ব্যবহারকারীর কথা:\\n{question}\\n\\nউত্তর:\\n\"\n    )\n\n    # Tokenize and send to GPU\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n    responses = []\n\n    for _ in range(n_samples):\n        with torch.no_grad():\n            output_ids = model.generate(\n                **inputs,\n                max_new_tokens=MAX_NEW_TOKENS,\n                do_sample=True,\n                temperature=0.35,\n                top_p=0.8,\n                repetition_penalty=1.2,\n                eos_token_id=tokenizer.eos_token_id,\n                pad_token_id=tokenizer.eos_token_id,\n            )\n\n        decoded = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n        response = decoded[len(prompt):].strip()\n        responses.append(response)\n\n    return responses\n\n\n# -------- LOAD QUESTIONS --------\ndf = pd.read_csv(INPUT_PATH)\nassert \"question\" in df.columns, \"CSV must contain a 'question' column\"\nquestions = df[\"question\"].tolist()\n\nresults = []\n\n# -------- GENERATE RESPONSES --------\nfor idx, question in enumerate(tqdm(questions, desc=\"Generating responses\")):\n    outputs = generate_responses(question, N_SAMPLES)\n\n    for i, resp in enumerate(outputs):\n        results.append({\n            \"question\": question,\n            \"sample_id\": i + 1,\n            \"response\": resp\n        })\n\n# -------- SAVE RESULTS --------\npd.DataFrame(results).to_csv(OUTPUT_PATH, index=False)\nprint(f\"Saved to {OUTPUT_PATH}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:47:33.668605Z","iopub.execute_input":"2026-02-25T09:47:33.669342Z","iopub.status.idle":"2026-02-25T09:53:47.012622Z","shell.execute_reply.started":"2026-02-25T09:47:33.669310Z","shell.execute_reply":"2026-02-25T09:53:47.011898Z"}},"outputs":[{"name":"stderr","text":"Generating responses: 100%|██████████| 5/5 [06:13<00:00, 74.66s/it]","output_type":"stream"},{"name":"stdout","text":"Saved to /kaggle/working/model_generations.csv\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"import pandas as pd\n\n# Save with UTF-8 encoding\ndf = pd.read_csv(\"/kaggle/working/model_generations.csv\")  # your existing results\ndf.to_csv(\"/kaggle/working/model_generations_utf8.csv\", index=False, encoding=\"utf-8-sig\")\n\nprint(\"Saved CSV with UTF-8 encoding!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-25T09:53:51.988832Z","iopub.execute_input":"2026-02-25T09:53:51.989437Z","iopub.status.idle":"2026-02-25T09:53:51.997155Z","shell.execute_reply.started":"2026-02-25T09:53:51.989401Z","shell.execute_reply":"2026-02-25T09:53:51.996391Z"}},"outputs":[{"name":"stdout","text":"Saved CSV with UTF-8 encoding!\n","output_type":"stream"}],"execution_count":14}]}